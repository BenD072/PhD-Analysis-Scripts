{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize, PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "# There is a total of 4511 tweets \n",
    "# Take 10% to form classifier and code rest (450)\n",
    "df = pd.read_excel(\"/Users/user/Documents/Python /Twitter Webscraping/Analysis Scripts for Thesis/Excel Files for Thesis/Test_Data_For_Classifier.xlsx\", headers = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split tweets into training and test set\n",
    "# Use 80% as training set and 20% as test set \n",
    "train_data, test_data, train_labels, test_labels = train_test_split(df['tweetText'], df['Labels'], test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360\n",
      "360\n"
     ]
    }
   ],
   "source": [
    "# Check data and labels are same length \n",
    "print(len(train_data))\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "youre going dahhhhn you schlaaaaaaag\n",
      "  (0, 12)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 71)\t1\n",
      "  (0, 87)\t1\n",
      "  (0, 184)\t1\n",
      "  (0, 185)\t1\n",
      "  (0, 409)\t2\n",
      "  (0, 446)\t1\n",
      "  (0, 451)\t1\n",
      "  (0, 759)\t1\n",
      "  (0, 807)\t1\n",
      "  (0, 1593)\t1\n",
      "  (0, 1835)\t1\n",
      "  (0, 1876)\t1\n",
      "  (0, 2041)\t1\n",
      "  (0, 2048)\t1\n",
      "  (0, 2690)\t1\n",
      "  (0, 2693)\t1\n",
      "  (0, 2707)\t1\n",
      "  (0, 2712)\t1\n",
      "  (0, 2780)\t1\n",
      "  (0, 2782)\t1\n",
      "  (0, 3037)\t1\n",
      "  (0, 3043)\t1\n",
      "  (0, 3653)\t1\n",
      "  :\t:\n",
      "  (0, 4768)\t1\n",
      "  (0, 4784)\t1\n",
      "  (0, 4821)\t1\n",
      "  (0, 4828)\t1\n",
      "  (0, 4978)\t4\n",
      "  (0, 4979)\t1\n",
      "  (0, 4982)\t1\n",
      "  (0, 5028)\t1\n",
      "  (0, 5058)\t1\n",
      "  (0, 5110)\t1\n",
      "  (0, 5113)\t1\n",
      "  (0, 5133)\t1\n",
      "  (0, 5135)\t1\n",
      "  (0, 5228)\t1\n",
      "  (0, 5241)\t1\n",
      "  (0, 5248)\t1\n",
      "  (0, 5261)\t1\n",
      "  (0, 5306)\t1\n",
      "  (0, 5313)\t1\n",
      "  (0, 5467)\t1\n",
      "  (0, 5482)\t1\n",
      "  (0, 5526)\t1\n",
      "  (0, 5544)\t1\n",
      "  (0, 5705)\t1\n",
      "  (0, 5714)\t1\n"
     ]
    }
   ],
   "source": [
    "# Create the counter to vectorize the data \n",
    "counter = CountVectorizer(ngram_range=(1,2))  # Create counter object using unigrams and bigrams\n",
    "counter.fit(train_data)  # Fit counter on the training set \n",
    "train_counts = counter.transform(train_data)  # Transform training set into count vectors\n",
    "test_counts = counter.transform(test_data)  # Transform testing set into count vectors\n",
    "print(train_data[1])\n",
    "print(train_counts[1])  # See what tweet looks like as a count vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Logistic Regression Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs') \n",
    "model.fit(train_counts, train_labels)\n",
    "predictions = model.predict(test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "# Create the classifier and train it\n",
    "classifier = MultinomialNB()  # Create classifier model\n",
    "classifier.fit(train_counts, train_labels)  # Fit the model to the vector counts of the training data\n",
    "predictions = classifier.predict(test_counts)  # Make a list of predictions from the vector counts of the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8555555555555555\n"
     ]
    }
   ],
   "source": [
    "# NB gives higher accuracy at 85%\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use classifier to predcit class for remaining tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "df2 = pd.read_excel(\"/Users/user/Documents/Python /Twitter Webscraping/Analysis Scripts for Thesis/Excel Files for Thesis/Remaining Boris Replies for Classification.xlsx\", headers = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of tweet text, classify and add back to dataframe\n",
    "tweets_to_predict = df2['tweetText']\n",
    "\n",
    "# Transform tweets into vector counts\n",
    "new_tweet_counts = counter.transform(tweets_to_predict)\n",
    "\n",
    "# Create a list of predicted labels for the new tweets\n",
    "new_predictions = classifier.predict(new_tweet_counts)\n",
    "\n",
    "# Add the labels back into the dataframe\n",
    "df2[\"Labels\"] = new_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets for supportive and unsupportive tweets\n",
    "# Split into two dataframes - for and agaisnt \n",
    "favour_tweets = []\n",
    "against_tweets = []\n",
    "\n",
    "for index, row in df2.iterrows():\n",
    "    if row['Labels'] == 1:\n",
    "        favour_tweets.append(row)\n",
    "    else:\n",
    "        against_tweets.append(row)\n",
    "        \n",
    "favour_tweets = pd.DataFrame(favour_tweets)\n",
    "against_tweets = pd.DataFrame(against_tweets)\n",
    "\n",
    "favour_tweets.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "against_tweets.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframes\n",
    "favour_tweets.to_excel('Supportive_Tweets_For_Johnson.xlsx')\n",
    "against_tweets.to_excel('Unsupportive_Tweets_For_Johnson.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
